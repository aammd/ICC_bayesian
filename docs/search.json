{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2022-06-08T07:15:20-04:00"
    },
    {
      "path": "index.html",
      "title": "Bayesian version of the Interclass Correlation Coefficient (ICC)",
      "description": "How much do groups contribute to variance in a response? Using posterior simulations to get a simple (?) answer.\n",
      "author": [],
      "contents": "\nHow much variation is contributed by group differences? This is a useful question in several contexts. Sometimes it is simply a guide to model building – is it “worth it” to include group effects in my model? In other projects we might care about measuring variation explicitly. For example, we might want to measure what proportion of phenotypic variation can be ascribed to different genotypes. Or we might have different experimental subjects, and we want to know if the same subject responds in the same way across trials.\nClearly, observations from the same subject will be correlated to each other, since all those observations come from the same person/animal/thing, which differs in all kinds of unmeasured ways from the other people/animal/things we are studying. This correlation is measured, for some models, using the “interclass correlation coefficient” (ICC). The trouble is that the ICC:\nDoesn’t have any uncertainty estimates – since it comes from a model, we have to represent a distribution of possible correlations consistent with the data\nOnly works sometimes. There are only a few models for which there are formulae to calculate ICC, and a wide class of flexible models are left out with no clear way to proceed.\nHere I’m suggesting an approach to calculate the ICC based on a Bayesian posterior distribution for the model coefficients. The approach is heavily based on the Bayesian R2 (Gelman et al. 2019). The approach I’m suggesting replaces a single value of ICC with a distribution. However, for models beyond simple intercept-only models, this ICC calculation produces a curve, \\(\\text{ICC}(x)\\), because in these cases the contribution of group effects depends on the rest of the model.\nBelow is a quick description of what I’m suggesting followed by some simulations testing it in a few interesting cases.\nWhat is the ICC\nThe ICC is the proportion of total variation that comes from group differences. The simplest and most intuitive formula comes from a gaussian model with random intercepts, like this one:\n\\[\n\\begin{align}\ny_i &\\sim \\text{Normal}(\\mu_i, \\sigma_{obs}^2) \\\\\n\\mu_i &= \\alpha_0 + X_i\\beta+ \\alpha_{\\text{group}[i]} \\\\\n\\alpha_{\\text{group}} &\\sim \\text{Normal}(0, \\sigma_{\\text{group}}^2)\n\\end{align}\n\\]\nThat is, a gaussian response with random intercepts for groups. In this case, \\(X_i\\beta\\) could be any vector of predictors, multiplied by their coefficients.\nHere, the ICC is a ratio of the hyperparameter \\(\\sigma_{\\text{group}}^2\\) to sum of that hyperparameter and the residual variance, \\(\\sigma_{\\text{obs}}^2\\).\n\\[\n\\frac{\\sigma_{group}^2}{\\sigma_{group}^2 + \\sigma_{obs}^2}\n\\]\nSince all variation in the above model comes from one of these two places, the result is the proportion of variation which comes from group differences.\nProposed simulation based version\nThe idea for an ICC based on simulations is inspired by the Bayesian R^2 equation of (Gelman et al. 2019), which is also explained in this excellent workbook by Gelman, Hill and Vehtari:\n\\[\nR^2 = \\frac{\\text{Var}_\\mu}{\\text{Var}_\\mu + \\text{Var}_{\\text{res}}}\n\\]\nHere \\(\\mu\\) is the model fitted values:\n\\[\n\\text{Var}_\\mu^s = V_{n=1}^N\\mu_n^s\n\\]\nWe also need the residual variance. To make it general, we use the expected residual variance like this:\n\\[\n\\text{Var}_{\\text{res}}^s = \\frac{1}{N}\\sum_{n=1}^N (\\text{Var}_{\\text{res}})_n^s\n\\]\nIn words: for every posterior draw \\(s\\), get the variance for each observation \\(n\\) and average them. In a gaussian regression with a constant variance you would of course get a constant:\n\\[\n\\frac{1}{N}\\sum_{n=1}^N (\\text{Var}_{\\text{res}})_n^s = \\frac{1}{N}\\sum_{n=1}^N (\\sigma^2)^s = (\\sigma^2)^s\n\\]\nBut that’s not true for example in a logistic regression (this is the example given in the workbook linked above), where the mean and the variance have a close relationship. If the mean for observation number \\(n\\) in draw \\(s\\) is \\(\\pi_n^s\\), then:\n\\[\n\\frac{1}{N}\\sum_{n=1}^N (\\text{Var}_{\\text{res}})_n^s = \\frac{1}{N}\\sum_{n=1}^N \\pi_n^s(1-\\pi_n^s)\n\\]\nIn this calculation, we look at the variance across the models fitted values and combine it with the expected variance for each fitted value.\nThe approach I’m suggesting is basically the same equation in two dimensions. The recipie goes like this:\nFor each posterior sample \\(\\theta^s\\), draw \\(K\\) groups from the hyperpriors that define the groups. Typically this will be a univariate or multivariate gaussian distribution. Each group has its id number: \\(1 ... k ... K\\). You end up with a matrix of group-level parameters, with dimensions \\(S\\) and \\(K\\).\nCreate a smooth vector of \\(x\\) values to predict across, within the range of your original \\(x\\) values. Make \\(M\\) of these values, \\(x_{\\text{min}} ... m ... x_{\\text{max}}\\).\nFor each value \\(m\\), AND for each posterior sample \\(s\\) calculate the variance across all the \\(K\\) groups when $ x = m $:\n\\[\n\\text{ICC}(x = m) = \\frac{\nV_{k=1}^K(\\mu_m)_k^s\n}{\nV_{k=1}^K(\\mu_m)_k^s + \\frac{1}{K}\\sum_{k=1}^K (\\text{Var}_m^{\\text{res}})_k^s\n}\n\\]\nWhat this snarled notation is meant to convey is: We can calculate the ICC in a very similar way to the bayes \\(R^2\\) value. In this case, instead of calculating the variance across the \\(X_n\\) observations, we calculate the variance across the \\(k\\) groups for some particular value of \\(x\\), say when \\(x = m\\).\n\\((\\mu_m)_k^s\\) is the expected value of the model when \\(x=m\\), for group \\(k\\) and posterior draw \\(s\\). For the model above this would be:\n\\[\n(\\mu_m)_k^s = \\alpha_0^s + m\\times\\beta^s+ \\alpha_{\\text{group }k}^s\n\\]\nWhere \\(m\\) is one particular value for \\(x\\). In all the models I’ve tried this on so far, there’s been a single univariate \\(x\\) value1. By calculating the variance across \\(k\\) values of \\((\\mu_m)_k^s\\), we get an estimate of how much groups contribute to overall variation at \\(x = m\\), according to posterior draw \\(s\\).\nResidual variance, \\(\\text{Var}_m^{\\text{res}}\\), is one constant number for any gaussian regression in a particular posterior draw \\(s\\). For most (all?) other regressions, it will typically be some function of \\(m\\). For example, in a logistic regression it would be \\(\\mu_m(1-\\mu_m)\\).\n\\(\\mu_m\\) is of course a function of \\(m\\), since \\[\n\\text{logit}(\\mu_m) = \\alpha_0 + m*\\beta + \\alpha_{\\text{group }k}\n\\]\nIn these cases it will also be a function of the group offsets, and so each group \\(k\\) will have its own residual variance. There might even be a model where residuals differ among groups (i.e. groups which are more or less “predictable,” with a greater or smaller variance). Because each of the \\(K\\) groups will have its own variance in these cases, we take the average.\n\nIs the mean a good way to summarize the expected value of residual variance for models where the different groups have very different variances?? Maybe the median would be a more sensible choice!\nThe goal of the rest of this document is to experiment with this calculation of \\(\\text{ICC}(x)\\). First I’ll compare it to previous means of calculating ICC, then extend it to cases where that calculation isn’t possible.\nEverything is based on simulated data.\nIn the notes below I’ll refer to three kinds of ICC calculation:\nFrequentist, AKA \\(ICC_F\\) using the point estimates from a ML model for the parameters and hyperparameters involved. This is the value produced by the rptR package.\nBayesian (analytic) AKA \\(\\text{ICC}_B\\) applying the same formula as the frequentist approach, but using posterior draws for the (hyper)parameters. This produces a posterior distribution of ICC values.\nBayesian (simulation) AKA \\(\\text{ICC}(x)\\), which is the approach I’m exploring here. This produces a value of the ICC for possible values of \\(x\\). The result can be visualized as a function of \\(x\\). For simple models this will be a flat line, with a marginal distribution that resembles that of \\(ICC_B\\). For all other models it will be a function with different values depending on \\(x\\)\n\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(lme4)\nlibrary(rptR)\nlibrary(brms)\nlibrary(tidyverse)\nlibrary(tidybayes)\n\n\n\nGaussian random intercepts model\nImagine a simple experiment: different genetic lines of the same plant are each exposed to different amounts of water. Their growth is normally distributed around an average determined by how much water they get. The different genetic lines are different in their growth but all respond precisely the same way to the addition of water:\n\n\nwater <- runif(50)\n\navg_size <- 23\n\necart_size <- rnorm(20, mean = 0, sd = 3)\n\nslope_water <- 4.2\nobs_sd <- 2\n\nobs_data <- expand_grid(sp_id = 1:20,\n            water) |>\n  mutate(avg = avg_size + ecart_size[sp_id] + slope_water*water, \n         obs = rnorm(length(avg), mean = avg, sd = obs_sd))\n\nobs_data |>\n  ggplot(aes(x = water, y = obs)) + \n  geom_point() +\n  geom_line(aes(y = avg, group = sp_id)) + \n  labs(x = \"Water\", y = \"Mass\")\n\n\n\n\nFrequentist calculation\nHow much of the variation in growth is contributed by plant lines? In other words, what proportion of total variance comes from genetic differences? We can apply the formula above by manually extracting the variance components from the model:\n\n\nlmer_mod <- lmer(obs~water + (1 | sp_id), data = obs_data)\n\ngauss_vc <- VarCorr(lmer_mod) |> as.data.frame()\n\nfreq_line <- gauss_vc$vcov[[1]]/(gauss_vc$vcov[[1]] + gauss_vc$vcov[[2]])\nfreq_line\n\n\n[1] 0.6057147\n\nWe can do the same with the rptR package (Stoffel, Nakagawa, and Schielzeth 2017)\n\n\nrpt_obs <- rpt(obs ~ water + (1 | sp_id),\n                     data = mutate(obs_data, sp_id = factor(sp_id)),\n          grname = \"sp_id\", datatype = \"Gaussian\", nboot = 0, npermut = 0)\n\nrpt_obs\n\n\n\n\nRepeatability estimation using the lmm method \n\nRepeatability for sp_id\nR  = 0.606\nSE =  NA \nCI = [NA, NA]\nP  = 2.37e-179 [LRT]\n     NA [Permutation]\n\nBayesian calculation (parametric)\nAs noted by (Nakagawa and Schielzeth 2010):\n\nThe advantage of using a Bayesian implementation such as MCMCglmm for estimating precisions, is that MCMC samples from the posterior distribution of the parameters (e.g. σ 2 α and σ 2 e ) can be combined (e.g. according to Eq. 22) to provide a valid posterior distribution for repeatability itself […]\n\nSo we can repeat the procedure above, but this time using posterior samples from the parameters. This gives us a distribution which we could use to obtain HPDI intervals or summarize in some other way.\nThe first step is to fit a bayesian model:\n\n\ngaussian_intercept_bf <- bf(obs~ water + (1 | sp_id),\n                            family = gaussian())\n\n# get_prior(gaussian_intercept_bf,\n#          data = obs_data)\n\ngaussian_intercept_priors <- c(\n  prior(exponential(.3), class = sigma),\n  prior(exponential(.2), class = \"sd\"),\n  prior(normal(5, 6), class = \"b\"),\n  prior(normal(50, 10), class = \"Intercept\")\n)\n\n\ngaussian_brm <- brm(gaussian_intercept_bf,\n                 prior = gaussian_intercept_priors,\n                 data = obs_data, file = \"gaussian_brm\",\n                 backend = \"cmdstanr\", cores = 4)\n\ngaussian_brm <- update(gaussian_brm, newdata = obs_data, cores = 4, refresh = 0)\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 4 finished in 7.0 seconds.\nChain 1 finished in 7.5 seconds.\nChain 2 finished in 8.1 seconds.\nChain 3 finished in 8.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 7.7 seconds.\nTotal execution time: 8.5 seconds.\n\nThe following creates the \\(K\\) groups for each posterior sample. I visualize one sample just to show that (1) they look just like the original groups (2) there’s a lot of them. At this stage it is absolutely critical if you are using brms that you set the option sample_new_levels = \"gaussian\". All this and more is described in Andrew Heiss’s fabulous blog on the topic\n\n\nsample_draws <- expand_grid(sp_id = paste0(\"newsp\", 1:100),\n                            water = unique(obs_data$water)) |>\n  add_epred_draws(gaussian_brm,\n                  allow_new_levels = TRUE,\n                  sample_new_levels= \"gaussian\")\n\nsample_draws |> \n  ungroup() |> \n  filter(.draw == 121) |> \n  ggplot(aes(x = water, y = .epred, group = sp_id)) + \n  geom_line(alpha = .3) + \n  theme_bw()\n\n\n\n\n(#fig:posterior_k_gauss)100 simulated groups from a single posterior sample of the hyperparameter \\(\\sigma_{\\text{group}}^2\\)\n\n\n\nIMPORTANT The next step is one that I tested a few times and I hope is valid. We need to match the average predictions from add_epred_draws to the \\(\\sigma_{\\text{res}}\\) values from those exact same draws. I think that this works – ie that if you take ALL the posterior draws (not using the ndraws argument) then the numbers of the draws match. If so, then the data frames can be merged together.2\n\n\nsample_variance <- sample_draws |>\n  ungroup() |>\n  nest_by(.draw, water) |>\n  mutate(Vpred = var(data$.epred))\n\nsigma_draws <- spread_draws(gaussian_brm, sigma)\n\nicc_of_x <- sample_variance |>\n  ungroup() |>\n  left_join(sigma_draws, by = \".draw\") |>\n  mutate(icc_x = Vpred/(Vpred + sigma^2))\n\n\n\nFinally, with this merged dataframe, we can calculate the Bayesian simulation approach (\\(\\text{ICC}(x)\\)) and compare it to the Bayesian analytic option (\\(\\text{ICC}_B\\)) and the frequentist point estimate (\\(\\text{ICC}_F\\))\n\n\n# get_variables(gaussian_brm)\n\ngaussian_varcomp <- gaussian_brm |> \n  spread_draws(sd_sp_id__Intercept, sigma)\n\ngaussian_varcomp |> \n  mutate(icc_analytic = sd_sp_id__Intercept^2/(sd_sp_id__Intercept^2 + sigma^2)) |> \n  ggplot(aes(x = icc_analytic))  +\n  geom_density(lwd = 2) + \n  geom_density(data = icc_of_x, aes(x = icc_x), col = \"orange\", lwd=2) + \n  geom_vline(xintercept = freq_line, col = \"darkgreen\", lwd=2) + \n  theme_bw() + \n  labs(x = \"ICC\")\n\n\n\n\nFigure 1: Comparison of three ways of calculating the ICC. The vertical green line is the frequentist point estimate. The black curve is the posterior distribution obtained by applying the formula to posterior samples of the hyperparameter and parameter. The orange curve is the simulation based approach for 100 simulated groups.\n\n\n\nThe pattern is quite similar, which is encouraging! If \\(\\text{ICC}(x)\\) gives the same answer as the other two, then it might be a useful tool. It is FAR more computationally demanding, but it is also much more flexible.\nOff script\nIn the following sections there are no more comparisons to \\(\\text{ICC}_B\\) or \\(\\text{ICC}_F\\), because (as far as I know) its not possible to calculate them for any of these models.\nGaussian random slopes\n\n\nwater <- runif(50, min = -1, max = 1)\n\navg_size <- 23\navg_eff_water <- 7\ncorrmat <- matrix(\n  c(1,-.8,\n    -.8, 1), nrow = 2, byrow = TRUE\n)\n\nsds <- c(1,4)\nvcov <- diag(sds) %*% corrmat %*% diag(sds)\n\nb <- MASS::mvrnorm(20, mu = c(0, 0), Sigma = vcov)\n\nobs_sd <- 1\n\nobs_data <- expand_grid(sp_id = 1:20,\n            water) |>\n  mutate(avg = avg_size + avg_eff_water*water +  b[sp_id,1] + b[sp_id,2]*water, \n         obs = rnorm(length(avg), mean = avg, sd = obs_sd))\n\nobs_data |>\n  ggplot(aes(x = water, y = obs)) + \n  geom_point(alpha = .6) +\n  geom_line(aes(y = avg, group = sp_id)) + \n  labs(x = \"Water\", y = \"Mass\")\n\n\n\n\nFit a bayesian model and plot the ICC(x)\n\n\ngaussian_slope_bf <- bf(obs~ 1+ water + (1 + water | sp_id),\n                            family = gaussian())\n\nget_prior(gaussian_slope_bf,\n         data = obs_data)\n\n\n                   prior     class      coef group resp dpar nlpar\n                  (flat)         b                                \n                  (flat)         b     water                      \n                  lkj(1)       cor                                \n                  lkj(1)       cor           sp_id                \n student_t(3, 23.3, 4.9) Intercept                                \n    student_t(3, 0, 4.9)        sd                                \n    student_t(3, 0, 4.9)        sd           sp_id                \n    student_t(3, 0, 4.9)        sd Intercept sp_id                \n    student_t(3, 0, 4.9)        sd     water sp_id                \n    student_t(3, 0, 4.9)     sigma                                \n bound       source\n            default\n       (vectorized)\n            default\n       (vectorized)\n            default\n            default\n       (vectorized)\n       (vectorized)\n       (vectorized)\n            default\n\ngaussian_slope_priors <- c(\n  prior(exponential(.3), class = sigma),\n  prior(exponential(.5), class = \"sd\"),\n  prior(lkj(2), class = \"cor\"),\n  prior(normal(5, 6), class = \"b\"),\n  prior(normal(20, 10), class = \"Intercept\")\n)\n\n\ngaussian_slope_brm <- brm(gaussian_slope_bf,\n                          prior = gaussian_slope_priors,\n                          data = obs_data, file = \"gaussian_slope_brm\",\n                          backend = \"cmdstanr\", cores = 4)\n\ngaussian_slope_brm <- update(gaussian_slope_brm, newdata = obs_data, cores = 4, refresh = 0)\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 2 finished in 44.9 seconds.\nChain 1 finished in 45.5 seconds.\nChain 3 finished in 46.0 seconds.\nChain 4 finished in 46.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 45.7 seconds.\nTotal execution time: 46.7 seconds.\n\ncalculate ICC(x)\n\n\ngauss_slope_sample_draws <- expand_grid(sp_id = paste0(\"newsp\", 1:100),\n                            water = seq(from = -1, to = 1, length.out = 33)) |>\n  add_epred_draws(gaussian_slope_brm,\n                  allow_new_levels = TRUE,\n                  sample_new_levels= \"gaussian\")\n\ngauss_slope_sample_draws |> \n  ungroup() |> \n  filter(.draw == 121) |> \n  ggplot(aes(x = water, y = .epred, group = sp_id)) + \n  geom_line(alpha = .3) + \n  theme_bw()\n\n\n\n\n\n\ngauss_slope_sample_variance <- gauss_slope_sample_draws |>\n  ungroup() |>\n  nest_by(.draw, water) |>\n  mutate(Vpred = var(data$.epred)) |> \n  select(-data)\n\nsigma_draws <- spread_draws(gaussian_slope_brm, sigma)\n\ngauss_slope_icc_of_x <- gauss_slope_sample_variance |>\n  ungroup() |>\n  left_join(sigma_draws, by = \".draw\") |>\n  mutate(icc_x = Vpred/(Vpred + sigma^2))\n\n\n\n\n\ngauss_slope_icc_of_x |> \n  ggplot(aes(x = water, y = icc_x)) + \n  stat_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\", direction = -1) + \n  labs(y = \"ICC(x)\", x = \"Water\")\n\n\n\n\nFigure 2: The ICC(x) for a random slopes model. In this case, the variation explained by groups is lowest in the middle of the gradient of X, and highest at the extremes.\n\n\n\nNonlinear models, normal errors\n\n\n# rates vary slightly\nlog_mean_rate <- log(0.7)\n\nlog_indiv_ecart <- rnorm(15, mean = 0, sd = .4)\n\nrates <- exp(log_mean_rate + log_indiv_ecart)\n\n## NORMAL OBS -----------------------------------\n\n## observations are normally distributed\n\nsd <- 5\nM0 <- 200\n\nsimulated_decay <- expand.grid(sample_id = 1:15, time = 0:10) |>\n  mutate(rate = exp(log_mean_rate + log_indiv_ecart[sample_id]),\n         Mt  = M0 * exp(-rate*time),\n         obs_mass = rnorm(length(Mt),\n                          mean = Mt,\n                          sd = sd))\n\nsimulated_decay |>\n  ggplot(aes(x = time, y = Mt, group = sample_id))+\n  geom_line()\n\n\n\ndata_figure <- simulated_decay |>\n  ggplot(aes(x = time, y = Mt, group = sample_id))+\n  geom_line() +\n  geom_point(aes(y = obs_mass))\n\n## brms mdeol\ndecay_normal_bf <- bf(obs_mass ~ 200*exp(-exp(rate) * time),\n   rate ~ 1 + (1 | sample_id), nl = TRUE, family = gaussian())\n\n#get_prior(decay_normal_bf, data = simulated_decay)\n\ndecay_normal_priors <- c(\n  prior(exponential(.3), class = sigma),\n  prior(exponential(.2), class = sd, nlpar = \"rate\"),\n  prior(normal(-.5, .5), class = \"b\", nlpar = \"rate\")\n)\n\n\ndecay_brm <- brm(decay_normal_bf,\n                 prior = decay_normal_priors,\n                 data = simulated_decay,\n                 file = \"decay_normal\",\n                 backend = \"cmdstanr\",\n                 cores = 4, refresh = 0)\n\ndecay_brm <- update(decay_brm, newdata = simulated_decay, cores = 4, refresh = 0)\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 3 finished in 3.7 seconds.\nChain 4 finished in 3.6 seconds.\nChain 2 finished in 3.9 seconds.\nChain 1 finished in 4.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 3.8 seconds.\nTotal execution time: 4.2 seconds.\n\n#### make fake groups\nnew_groups <- paste0(\"s\", 1:33)\n\nsample_draws <- expand_grid(sample_id = new_groups,\n                            time = seq(from = .1,\n                                       to = 10,\n                                       length.out = 40)) |>\n  add_epred_draws(decay_brm,\n                      allow_new_levels = TRUE,\n                      sample_new_levels= \"gaussian\")\n\nsample_draws |>\n  ungroup() |>\n  filter(.draw<5) |>\n  ggplot(aes(x = time, y = .epred, group = sample_id)) +\n  geom_line() +\n  facet_wrap(~.draw)\n\n\n\n# for each draw, at each value of x,\n# calculate the variance of the predictions\n\nsample_variance <- sample_draws |> ungroup() |>\n  nest_by(.draw, time) |>\n  mutate(Vpred = var(data$.epred))\n\nsigma_draws <- spread_draws(decay_brm, sigma)\n\nicc_of_x <- sample_variance |>\n  ungroup() |>\n  select(.draw,time, Vpred) |>\n  left_join(sigma_draws, by = \".draw\") |>\n  mutate(icc_x = Vpred/(Vpred + sigma^2))\n\nicc_figure <- icc_of_x |>\n  ggplot(aes(x = time, y = icc_x)) +\n  stat_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\", direction = -1) + \n  labs(y = \"ICC(x)\", x = \"Time\")\n\nlibrary(patchwork)\n\ndata_figure + icc_figure\n\n\n\n\nDecay with Gamma errors\n\n\nalpha <- 50\n\nlog_mean_rate <- log(0.7)\n\nlog_indiv_ecart <- rnorm(15, mean = 0, sd = .7)\n\nsimulated_decay_gamma <- expand.grid(sample_id = 1:15, time = 0:10) |>\n  mutate(rate = exp(log_mean_rate + log_indiv_ecart[sample_id]),\n         Mt  = 200 * exp(-rate*time),\n         obs_mass = rgamma(length(Mt),shape = alpha, rate = alpha/Mt)) |>\n  mutate(sample_id = paste0(\"samp\", sample_id))\n\n# replace 0s with smallest nonzero\ntinymass <- with(simulated_decay_gamma, min(obs_mass[obs_mass>0]))\n\nsimulated_decay_gamma <- simulated_decay_gamma |>\n  mutate(obs_mass = if_else(obs_mass == 0, tinymass, obs_mass))\n\ngamma_data_figure <- simulated_decay_gamma |>\n  ggplot(aes(x = time, y = Mt, group = sample_id ))+\n  geom_line() +\n  geom_point(aes(y = obs_mass))\n\n\n\n\n\ndecay_gamma_bf <- bf(\n  obs_mass ~ 200*exp(-exp(rate) * time),\n  rate ~ 1 + (1 | sample_id),\n  nl = TRUE,\n  family = brmsfamily(\"Gamma\",\n                      link = \"identity\",\n                      link_shape = \"log\")) +\n  lf(shape ~ 1)\n\n#get_prior(decay_gamma_bf, data = simulated_decay_gamma)\n\ndecay_gamma_priors <- c(\n  prior(exponential(4), class = \"sd\", nlpar = \"rate\"),\n  prior(normal(-.5, .5), class = \"b\", nlpar = \"rate\"),\n  prior(normal(4, 1), class = \"Intercept\", dpar  = \"shape\")\n)\n\n\ndecay_gamma_brm <- brm(decay_gamma_bf,\n                 prior = decay_gamma_priors,\n                 data = simulated_decay_gamma,\n                 file = \"decay_gamma_brm\",\n                 backend = \"cmdstanr\", cores = 4,\n                 sample_prior = \"yes\", refresh = 0)\n\ndecay_gamma_brm <- update(decay_gamma_brm,\n                          newdata = simulated_decay_gamma, \n                          cores = 4, refresh = 0)\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 2 finished in 47.6 seconds.\nChain 4 finished in 50.4 seconds.\nChain 1 finished in 50.1 seconds.\nChain 3 finished in 51.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 49.8 seconds.\nTotal execution time: 52.0 seconds.\n\n\n\ndecay_epred_draws <- expand_grid(time = seq(from = .1,\n                                            to = 10,\n                                            length.out = 40),\n                                 sample_id = letters) |>\n  add_epred_draws(decay_gamma_brm,\n                  allow_new_levels = TRUE,\n                  sample_new_levels= \"gaussian\")\n\n\n# calculate the variance among groups for each value of x (here x is time)\ndecay_var_epred <- decay_epred_draws |>\n  ungroup() |>\n  nest_by(.draw, time) |>\n  mutate(Vpred = var(data$.epred))\n\n# get the shape.. first what was it called\n# decay_gamma_brm |> get_variables()\n\nshape_draws <- spread_draws(decay_gamma_brm, Intercept_shape)\n\n# stancode(decay_gamma_brm)\n# need to exponentiate the shape draws as well!\n\ndecay_gamma_mu_shape <- decay_var_epred |>\n  ungroup() |>\n  left_join(shape_draws, by = \".draw\")\n\n# calculate variance and then calculate icc\nicc_gamma <- decay_gamma_mu_shape |>\n  rowwise() |>\n  # exponentiante shape and divide the square of the mean by that\n  mutate(shape = exp(Intercept_shape),\n         mu_sq = list(data$.epred^2),\n         var_k = list(mu_sq / shape),\n         mean_var = mean(var_k))\n\n\nclean_icc <- icc_gamma |>\n  mutate(icc_x = Vpred/(Vpred + mean_var)) |>\n  select(time, .draw, icc_x) |>\n  ungroup()\n\ngamma_icc_fig <- clean_icc |>\n  ggplot(aes(x = time, y = icc_x)) +\n  stat_lineribbon() + \n  scale_fill_brewer(palette = \"Greens\", direction = -1) + \n  labs(y = \"ICC(x)\", x = \"Time\")\n\n\n\n\n\ngamma_data_figure + gamma_icc_fig\n\n\n\n\nFigure 3: ICC for an exponential decay model with gamma errors. Because the variance is proportionally smaller than the mean, as the mean goes to 0 it takes the variance with it (but even faster). The result is that soon group ID controls almost all of the variation. This variation however is practically 0, since and this point almost everything has decayed to 0\n\n\n\nTwo models, each with very comparable structures, give completely opposite ICC(x) curves. Its an interesting consequence of extending the ICC in this way.\nDiscussion\nWhat’s this good for? This extension of the classic ICC approach can apply to a much wider range of models, but it is also more complex to interpret. For example, there’s no clear way to put a single number on the outcome. For all but the simplest models, it is no longer possible to make a statment like “repeatability of this behaviour among animals was 60%.” Instead, we’d have to say something like “repeatability reached a maximum value of 60% (HDPI between 51 and 67%) after 5 days, after which it declined,” or “the ICC reached a value greater than 90% when water addition increased beyond 60L per m2”\nBecause every quantity from a bayesian model has a posterior distribution, this approach would let us calculate both repeatability and broad-sense heritability (two interpretations of the ICC) for model-derived quantities. For example, we could expose many clonal lines of, say, aphids to different temperatures and measure population growth rates for each line at each temperature. This data could be modelled with thermal performance curves, each of which has several parameters. These parameters would be drawn from a multivariate normal distribution that would describe the variation among genotypes (clonal lines). With the resulting model it would be possible to measure the broad-sense heritability of, say, the temperature at which population growth rates are maximized.\n\n\n\nGelman, Andrew, Ben Goodrich, Jonah Gabry, and Aki Vehtari. 2019. “R-Squared for Bayesian Regression Models.” The American Statistician 73 (3): 307–9. https://doi.org/10.1080/00031305.2018.1549100.\n\n\nNakagawa, Shinichi, and Holger Schielzeth. 2010. “Repeatability for Gaussian and Non-Gaussian Data: A Practical Guide for Biologists.” Biological Reviews 85 (4): 935–56. https://doi.org/10.1111/j.1469-185X.2010.00141.x.\n\n\nStoffel, Martin A., Shinichi Nakagawa, and Holger Schielzeth. 2017. “rptR: Repeatability Estimation and Variance Decomposition by Generalized Linear Mixed-Effects Models.” Methods in Ecology and Evolution 8: 1639???1644. https://doi.org/10.1111/2041-210X.12797.\n\n\nI think it should be possible to extend this to multiple X values, in which case the ICC would become not a curve but a surface!↩︎\nI will probably have to ask this question of the creator of the wonderful tidybayes, Matthew Kay↩︎\n",
      "last_modified": "2022-06-09T17:56:12-04:00"
    }
  ],
  "collections": []
}
